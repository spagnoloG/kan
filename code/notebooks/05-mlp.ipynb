{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Backpropagation\n",
    "\n",
    "## Forward Pass\n",
    "\n",
    "Let’s assume a neural network with two layers. For simplicity, let:\n",
    "    1. $\\mathbf{x}$ \\in $\\mathbb{R}^n$ be the input vector.\n",
    "    2. $\\mathbf{W}_1$ \\in $\\mathbb{R}^{m \\times n}$ and $\\mathbf{b}_1$ \\in $\\mathbb{R}^m$ be the weight matrix and bias vector for the first layer.\n",
    "    3. $\\mathbf{W}_2$ \\in $\\mathbb{R}^{k \\times m}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^k$ be the weight matrix and bias vector for the second (output) layer.\n",
    "    4. $\\sigma(\\cdot)$ is an activation function (such as sigmoid or ReLU).\n",
    "\n",
    "### Step 1: First Layer Computations\n",
    "1. Compute the weighted sum of inputs for the first layer:\n",
    "    $$\n",
    "   \\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1\n",
    "   $$\n",
    "2. Apply the activation function to get the activations of the first layer:\n",
    "    $$\n",
    "   \\mathbf{a}_1 = \\sigma(\\mathbf{z}_1)\n",
    "    $$\n",
    "\n",
    "###  2: Second Layer Computations (Output Layer)\n",
    "1. Compute the weighted sum for the second layer:\n",
    "    $$\n",
    "   \\mathbf{z}_2 = \\mathbf{W}_2 \\mathbf{a}_1 + \\mathbf{b}_2\n",
    "   $$\n",
    "\n",
    "2. Apply the activation function (or softmax if it’s for classification) to get the final output:\n",
    "   $$\n",
    "   \\mathbf{y} = \\sigma(\\mathbf{z}_2)\n",
    "   $$\n",
    "\n",
    "Here, $\\mathbf{y}$ is the network’s prediction for a given input $\\mathbf{x}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Backpropagation Using Chain Rule\n",
    "\n",
    "Let’s assume a loss function $L(\\mathbf{y}, \\mathbf{t})$, where $\\mathbf{t}$ is the target output vector. We’ll backpropagate to update $\\mathbf{W}_2$, $\\mathbf{b}_2$, $\\mathbf{W}_1$, and $\\mathbf{b}_1$ using gradients obtained via the chain rule.\n",
    "\n",
    "### Step 1: Compute the Derivative of the Loss with respect to $\\mathbf{z}_2$\n",
    "\n",
    "1. The loss gradient with respect to the output $\\mathbf{y}$ is:\n",
    "    $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{y}} = \\nabla_{\\mathbf{y}} L\n",
    "   $$\n",
    "2. Using the chain rule, compute the derivative of L with respect to $\\mathbf{z}_2$:\n",
    "    $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{z}_2} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\circ \\sigma'(\\mathbf{z}_2)\n",
    "   $$\n",
    "   where $\\circ$ denotes element-wise multiplication and $\\sigma'(\\mathbf{z}_2)$ is the derivative of the activation function applied element-wise to $\\mathbf{z}_2$.\n",
    "\n",
    "### Step 2: Derivatives with respect to $\\mathbf{W}_2$ and $\\mathbf{b}_2$\n",
    "\n",
    "1. Using the chain rule, compute the gradient of the loss with respect to $\\mathbf{W}_2$:\n",
    "    $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{W}_2} = \\frac{\\partial L}{\\partial \\mathbf{z}_2} \\cdot \\mathbf{a}_1^\\top\n",
    "   $$\n",
    "2. Similarly, compute the gradient with respect to $\\mathbf{b}_2$:\n",
    "    $$  \n",
    "    \\frac{\\partial L}{\\partial \\mathbf{b}_2} = \\frac{\\partial L}{\\partial \\mathbf{z}_2}\n",
    "    $$\n",
    "\n",
    "#### Step 3: Backpropagate to the First Layer\n",
    "\n",
    "Now, propagate the error back to the first layer to compute the gradients with respect to $\\mathbf{W}_1$ and $\\mathbf{b}_1$.\n",
    "\n",
    "1. Compute the error at the first layer by backpropagating $\\frac{\\partial L}{\\partial \\mathbf{z}_2}$ through $\\mathbf{W}_2$:\n",
    "    $$  \n",
    "   \\frac{\\partial L}{\\partial \\mathbf{z}_1} = (\\mathbf{W}_2^\\top \\cdot \\frac{\\partial L}{\\partial \\mathbf{z}_2}) \\circ \\sigma'(\\mathbf{z}_1)\n",
    "   $$\n",
    "2. Compute the gradient of the loss with respect to $\\mathbf{W}_1$:\n",
    "    $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{W}_1} = \\frac{\\partial L}{\\partial \\mathbf{z}_1} \\cdot \\mathbf{x}^\\top\n",
    "   $$\n",
    "3. Compute the gradient with respect to $\\mathbf{b}_1$:\n",
    "    $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{b}_1} = \\frac{\\partial L}{\\partial \\mathbf{z}_1}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Backpropagation Updates\n",
    "\n",
    "With the gradients computed, update each parameter using a learning rate \\eta:\n",
    "\n",
    "1. Update weights and biases for the second layer:\n",
    "    $$\n",
    "   \\mathbf{W}_2 := \\mathbf{W}_2 - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}_2}\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{b}_2 := \\mathbf{b}_2 - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}_2}\n",
    "    $$\n",
    "\n",
    "2. Update weights and biases for the first layer:\n",
    "    $$\n",
    "   \\mathbf{W}_1 := \\mathbf{W}_1 - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}_1}\n",
    "   $$\n",
    "   $$\n",
    "   \\mathbf{b}_1 := \\mathbf{b}_1 - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}_1}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers, learning_rate=0.1, batch_size=32):\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.weights = [\n",
    "            np.random.normal(0.0, pow(layers[i], -0.5), (layers[i + 1], layers[i]))\n",
    "            for i in range(len(layers) - 1)\n",
    "        ]\n",
    "        self.biases = [np.zeros((layers[i + 1], 1)) for i in range(len(layers) - 1)]\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def sigmoid_prime(self, a):\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = np.array(inputs, ndmin=2)\n",
    "        activations = [inputs]\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            z = np.dot(activations[-1], w.T) + b.T\n",
    "            if i == len(self.weights) - 1:\n",
    "                # Output layer uses linear activation\n",
    "                a = z\n",
    "            else:\n",
    "                a = self.sigmoid(z)\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "\n",
    "    def cost_function(self, y, t):\n",
    "        return 0.5 * np.sum((y - t) ** 2)\n",
    "\n",
    "    def cost_function_prime(self, y, t):\n",
    "        return y - t\n",
    "\n",
    "    def backprop(self, X, y, activations):\n",
    "        batch_size = X.shape[0]\n",
    "        errors = [self.cost_function_prime(activations[-1], y)]\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            if i == len(self.weights) - 1:\n",
    "                # Output layer derivative is 1\n",
    "                delta = errors[-1]\n",
    "            else:\n",
    "                delta = errors[-1] * self.sigmoid_prime(activations[i + 1])\n",
    "            grad_w = np.dot(delta.T, activations[i]) / batch_size\n",
    "            grad_b = np.mean(delta, axis=0, keepdims=True).T\n",
    "            errors.append(np.dot(delta, self.weights[i]))\n",
    "            self.weights[i] -= self.learning_rate * grad_w\n",
    "            self.biases[i] -= self.learning_rate * grad_b\n",
    "        errors.reverse()\n",
    "\n",
    "    def fit(self, inputs_list, targets_list):\n",
    "        fit_loss = 0\n",
    "        for i in range(0, len(inputs_list), self.batch_size):\n",
    "            inputs_batch = np.array(inputs_list[i : i + self.batch_size], ndmin=2)\n",
    "            targets_batch = np.array(targets_list[i : i + self.batch_size], ndmin=2)\n",
    "\n",
    "            activations = self.forward(inputs_batch)\n",
    "            self.backprop(inputs_batch, targets_batch, activations)\n",
    "            fit_loss += self.cost_function(activations[-1], targets_batch)\n",
    "\n",
    "        fit_loss /= len(inputs_list)\n",
    "\n",
    "    def predict(self, inputs_list):\n",
    "        return self.forward(inputs_list)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare its performance with scikit implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Mean Squared Error: 0.32\n",
      "scikit-learn Neural Network Mean Squared Error: 0.33\n",
      "Linear Regression Mean Squared Error: 0.56\n"
     ]
    }
   ],
   "source": [
    "data = fetch_california_housing()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"].reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "y_train = scaler_y.fit_transform(y_train).ravel()  # Flatten for scikit-learn compatibility\n",
    "y_test = scaler_y.transform(y_test).ravel()\n",
    "\n",
    "# Train NN \n",
    "nn = NN(layers=[8, 20, 10, 1], learning_rate=0.1)\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    nn.fit(X_train, y_train.reshape(-1, 1))  # Reshape to keep consistency\n",
    "\n",
    "predictions_nn = []\n",
    "for x in X_test:\n",
    "    output = nn.predict(x)\n",
    "    predictions_nn.append(output[0, 0])\n",
    "\n",
    "predictions_nn = scaler_y.inverse_transform(np.array(predictions_nn).reshape(-1, 1))\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "mse_nn = mean_squared_error(y_test_original, predictions_nn)\n",
    "print(f\"Neural Network Mean Squared Error: {mse_nn:.2f}\")\n",
    "\n",
    "# Train scikit-learn Neural Network (MLPRegressor)\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(20, 10), learning_rate_init=0.1, max_iter=100, random_state=42)\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "predictions_mlp = mlp_reg.predict(X_test).reshape(-1, 1)\n",
    "predictions_mlp = scaler_y.inverse_transform(predictions_mlp)\n",
    "\n",
    "mse_mlp = mean_squared_error(y_test_original, predictions_mlp)\n",
    "print(f\"scikit-learn Neural Network Mean Squared Error: {mse_mlp:.2f}\")\n",
    "\n",
    "# Train Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "predictions_lr = linear_reg.predict(X_test)\n",
    "predictions_lr = scaler_y.inverse_transform(predictions_lr.reshape(-1, 1))\n",
    "\n",
    "mse_lr = mean_squared_error(y_test_original, predictions_lr)\n",
    "print(f\"Linear Regression Mean Squared Error: {mse_lr:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
